---
title: "week_3_exercises"
author: "Lindsey Greenhill"
date: "2/10/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arm)
library(tidyverse)
```


## Question 1

```{r data, include=FALSE}

df <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/pyth/exercise2.1.dat",
                 header = TRUE)

# using head to get the first 40 observations

df_train <- df %>% head(40)

# using tail to get the last 20 observations

df_test <- df %>% tail(20) %>%
  select(x1,x2)


fit_1 <- lm(y ~ x1 + x2, data = df_train)
```

### Part a

```{r 1a, echo=FALSE}

# the intercept is equal to 1.31, the x1 coefficient is equal to .514, the x2
# coefficient is equal to .806. In context this means that the average value of
# y when x1 and x2 are both 0 is 1.31. For every 1 unit increase in x1, y
# increases by .514 on average, holding x2 constant. For every 1 unit increase
# in x2, y increased by .806 on average, holding x1 constant. The R squared
# value is about .97, meaning that the model accounts for about 97% of
# variation.

display(fit_1)
```

The intercept is equal to 1.31, meaning that the average value of y when x1 and x2 are both 0 is 1.13. The coefficient for x1 is .514, meaning that for every 1 unit increase in x1, y increases by .514 on average,  holding x2 constant. The coefficient for x2 is .806, meaning that for every 1 unit increase in x2, y increases by .806 on average, holding x1 constant. 

The R squared value is .97, meaning that about 97% of the variance is explained by the model 

### Part b

The graphs below visualize the relationship between y and x1 and x2. The blue line is the least squared regression line. 

```{r 1b, echo=FALSE}

# plots of y vs x1 and x2

ggplot(df_train, aes(x = x1, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Y vs x1") +
  theme_classic()

ggplot(df_train, aes(x = x2, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Y vs  x2") +
  theme_classic()

```

### Part c

The plots below show the residuals of the two above models plotted against the observed values from the data frame. For the x1 plot, the residuals seem to be relatively evenly spread throughout the x1 values with no clear patterns emerging. For the x2 plot, the residuals seems to be larger for the x2 values closer to 0. However, the residuals for x2 seem to be smaller on average  than the residuals for x1. 

```{r 1c, echo=FALSE}

# doing residual plots. These plots show the actual values of x1 plotted against
# the residual of x1 in the above model. The horizontal line is at 0, meaning
# that the observed x1 and the predicted x1 were the same.

x1_fit <- lm(y ~ x1, data = df_train)
x1_res <- resid(x1_fit)
x2_fit <- lm(y ~ x2, data = df_train)
x2_res <- resid(x2_fit)

{
plot(df_train$x1, x1_res,
                    ylab = "Residuals",
                    xlab = "x1",
                    main = "x1 Residuals")
abline(0,0)
}

{
plot(df_train$x2, x2_res,
                    ylab = "Residuals",
                    xlab = "x2",
                    main = "x2 Residuals")
abline(0,0)
}
```

### Part d


```{r  1d, echo=FALSE}

# making predictions for the test data

predict(fit_1, newdata = df_test, interval = "prediction")

```


## Question 3

```{r 3a , echo=FALSE}

# this code pretty much all came from the book

var1 <- rnorm(1000,0,1)
var2 <- rnorm(1000,0,1)

fit_3 <- lm(var1 ~ var2)

# the coefficients are not statistically significant

summary(fit_3)

```

### Part a

The slope is not statistically significant. 

### Part b

```{r 3b, echo=FALSE}
z.scores <- rep (NA, 100)

for (k in 1:100) { 
  var1 <- rnorm (1000,0,1)
  var2 <- rnorm (1000,0,1) 
  fit <- lm (var2 ~ var1) 
  z.scores[k] <- coef(fit)[2] / se.coef(fit)[2]
} 

# counting the amount of values that are statistically significant

sum_z <- sum(z.scores >= 2)
```

Out of the 100 simulations, `r sum_z` estimates are statistically significant. 

